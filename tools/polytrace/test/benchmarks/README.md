# Benchmarks

This directory contains benchmark descriptions and a small runner used to execute `bench_program` scenarios and collect run artifacts.

## Building

From the repository root you can configure and build the polytrace project; the `bench_program` target is part of the Make build. The built binary will be available at:

```
tools/polytrace/build/bin/bench_program
```

## Configuration

`bench_program <path>` loads the JSON configuration from the path with following supported JSON keys and their types:

- `mode` (string): one of `write`, `read`, `create`, `unlink`, `cpu`, `sleep`
- `path` (string): directory path used for file operations (created if missing)
- `files` (integer): number of files per worker
- `file_size` (integer): total bytes written/read per file
- `write_size` (integer): chunk size for each write/read syscall
- `iterations` (integer): number of iterations per worker
- `threads` (integer): number of worker threads (ignored if `fork_children` > 0)
- `fork_children` (integer): number of forked child processes to spawn (useful for multi-PID tracing)
- `fsync_after_write` (boolean): call `fsync` after each file write
- `sleep_ms` (integer): milliseconds to sleep between iterations
- `cpu_ms` (integer): milliseconds of busy CPU work per iteration

Note: reading/writing files will create files named `bench_w<worker_id>_<file_index>` in the specified `path`. So a before running in "read" mode, make sure the files exist (e.g. by running first in "write" mode).


## Usage:

- `run_bench.sh` - simple runner script. It runs all given benchmark-JSON files in the directory and collects results into a CSV file. (run and install `shellcheck` to check the script)

- `plot_results.gnuplot` - gnuplot script to plot results from the CSV file generated by `run_bench.sh`. Install gnuplot and run `gnuplot plot_results.gnuplot` to generate a PNG plot.

# PolySim Benchmarks – Overhead vs. Wall-Time

This document describes how scaling of overheads relative to wall-time is analyzed and visualized. Goal: Post-processing should scale approximately as 1/x with runtime; tracing depends heavily on I/O and metadata intensity (with fixed I/O volume, also approximately 1/x).

## Overview
- Data source: Polytrace benchmark measurements in CSV format.
- Preparation: Extraction into a compact CSV for plots.
- Visualization: Log–Log scatter plots including 1/x reference and fits per category.
- Frontend display: The /benchmarks page renders the newly generated plots.

## Requirements
- Python 3.
- Python packages: matplotlib, numpy (install if needed):

```bash
python3 -m pip install matplotlib numpy
```

## File Paths
- Input (Benchmark results): tools/polytrace/test/benchmarks/results.csv
- Derived input for plots: tools/polytrace/test/benchmarks/runtime_overhead.csv
- Output (Plots for web): public/benchmarks/*.png

## CSV Formats
- results.csv (from benchmarks):
  - Columns: `run_id, wall_ms_traced, traced_ms, postproc_ms, wall_ms_untraced`
- runtime_overhead.csv (for plots, generated):
  - Columns: `runtime_ms, postproc_pct, tracing_pct, writing_level, benchmark_id`
  - Definitions:
    - `runtime_ms`: untraced runtime in ms (x-axis)
    - `postproc_pct`: (postproc_ms / wall_ms_untraced) * 100
    - `tracing_pct`: ((wall_ms_traced - wall_ms_untraced) / wall_ms_untraced) * 100
    - `writing_level`: heuristic for I/O intensity: "little" or "much"

## Steps: Run, Extract, Plot
1) Run benchmarks (creates results.csv):

```bash
cd tools/polytrace/test/benchmarks
./run_bench.sh
```

**Note: The benchmarks may take a while ("long" CPU times for better estimation)**

2) Extract compact CSV and scaling check:

```bash
python3 tools/polytrace/test/benchmarks/extract_runtime_overhead.py
```

Result: tools/polytrace/test/benchmarks/runtime_overhead.csv is generated and a brief 1/x analysis (product check) is printed to the console.

3) Generate plots:

```bash
python3 tools/polytrace/test/benchmarks/plot_benchmarks.py
```

Result (in public/benchmarks/):
- scaling_postproc_vs_runtime.png – Post-processing vs. runtime, log–log, fit slope and 1/x reference
- scaling_tracing_vs_runtime.png – Tracing vs. runtime, log–log, colored by writing level with fit slopes and 1/x reference
- benchmark_runtime_overhead.png – Legacy/PoC variant as supplementary

The /benchmarks page displays both scaling plots.

## Plot Interpretation
- 1/x expectation: In log–log representation, 1/x corresponds to a slope of approximately −1.
- Product check: For perfect 1/x, `runtime_ms × overhead%` remains constant. Large deviations indicate non-1/x behavior.
- Post-processing: Expected near 1/x (predominantly fixed proportion relative to wall-time).
- Tracing:
  - "little" writing: may be closer to 1/x if the number of syscalls is fixed.
  - "much" writing: often deviates if I/O volume scales with runtime or many metadata operations occur.

## Heuristic: Writing Level
The classification in tools/extract_runtime_overhead.py is based on benchmark ID:
- much: 02_io_heavy, 03_many_small_files, 04_write_then_read
- little: 01_long_run, 05_mixed_phases (and other unrecognized IDs)

## Tips
- New measurements: Re-run scripts 1) and 2); the /benchmarks page accesses the PNGs in public/benchmarks/.
- Optional: Create a Make or NPM task that runs both steps sequentially.